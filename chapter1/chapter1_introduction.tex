%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%% Static scene depthmap estimation determines a view-dependent depthfield by leveraging the local photoconsistency of a set of overlapping images observing a common scene. 

\chapter{Introduction}

Imagery records what the world looks like by projecting the 3D scene onto an image plane. However, the 3D information, which depicts the geometry of real objects, is lost during this capture process. Conversely,   3D information is key to many applications, such as augmented/virtual reality \cite{ventura2008depth}, robots and autonomous car navigation \cite{endres2012evaluation}, image-based rendering \cite{View_interpolation1993}, and image enhancement \cite{zhang2014personal}. Moreover, as additional information to RGB (Red, green, and blue) colors, 3D information is leveraged to improve performance of many computer vision tasks such as object classfication/recognition \cite{gupta2013perceptual}  and human pose estimation \cite{CVPR_kinect}. Therefore, there is a strong desire to recover reliable 3D information from 2D imagery.

3D information, when stored in computers, can be represented using 3D point clouds, 3D Polygon meshes or depthmaps. A 3D point cloud is a set of data points in three-dimensional space representing the external surface of an object, and it can be classified as either dense or sparse based on its density. In contrast, a 3D polygon mesh provides additional information in the form of the geometric topology among the 3D points. Compared to these representations which uses values in a global coordinate system, a depthmap is a dense field of depth values indicating the distance of the observed surface relative to a camera. In practice, different representations are adopted according to the requirements of specific applications. 

3D reconstruction from imagery, defined as a process that recovers 3D information from 2D image colors, is a traditional problem in 3D computer vision. Unlike the task of computer graphics that renders 2D imagery from 3D geometry, the inverse process of 3D reconstruction from imagery is more challenging since aiming at recovery of lost information inevitably introduces more ambiguities. Though 3D reconstruction has received heavy study and improved over the last few decades, it still remains a viable research area. This dissertation primarily focuses on the problems of dense static scene reconstruction and sparse dynamic object reconstruction from 2D imagery.

\textbf{Dense static scene reconstruction}.
To attain the 3D information of a static scene, most existing works leverage 2D correspondences and available camera parameters for 3D triangulation. Though camera parameters can typically be estimated by structure from motion or offline calibration methods, obtaining 2D correspondences robustly from image colors still requires further exploration. The 2D correspondences are defined as pixels in different images that observe the same part of a 3D scene. Under the assumption of a Lambertian surface, these 2D correspondences share the same or similar appearances/colors, and hence they have high color consistency.

For each point in one image, finding its correspondence in another image is attained by searching for candidate pixels with best color consistency along a line defined by the 3D geometry (called an epipolar line), and the positions of candidate pixels are determined by depth hypotheses generated in a valid range. Once the correspondence is found, the depth of the corresponding pixel is uniquely determined. However, estimating dense correspondences robustly is difficult since ambiguities arise in the cases of repetitive textures, homogeneous color regions, or occlusions along the epipolar line. 

Recently, there is a growing interest in using the ever-increasing crowd sourced data (\ie~Internet collected photos) for reconstruction, since the large amount of free data has inspired many applications such as virtual photo tours \cite{Snavely2} and image enhancement \cite{zhang2014personal}. With the non-controlled imagery as input, finding 2D correspondences based on colors is more challenging due to a diversity of factors such as heterogeneous resolution and scene illuminations, unstructured viewing geometry, scene content variability and image registration errors. To address these issues, it is normally assumed in the massive number of images, there are a subset of images sharing similar image characteristics. Therefore, determining a suitable subset of images or pixels for correspondence search becomes essential \cite{Goesele07}.

Dense reconstruction typically has very high computational complexity, since the traditional process involves exhaustive evaluations of a large number of depth hypotheses \cite{yang2003multi}. The increasing availability of crowd-sourced datasets has explicitly brought efficiency and scalability to the forefront of application requirements.  Moreover, the high complexity of a method would impede its usage in less-powerful electronic devices such as smart phones. To this end, there is a compelling demand to develop efficient and scalable methods for dense reconstruction.

\textbf{Sparse dynamic object reconstruction}.
The problem of dynamic object reconstruction specifically aims at 3D reconstruction under the circumstance of non-concurrent image captures. To be more precise, the dynamic object is only observed by one image at each time instance. This poses an additional challenge compared to the problem of static  scene reconstruction, since 3D triangulation becomes invalid and impossible with the unitary observation, even assuming 2D correspondences among non-concurrent images are correctly found. Given a unitary observation, it is only known the 3D point lies somewhere along the viewing ray determined by the 2D meansurement and the camera pose, but the depth along the viewing ray cannot be easily computed. Primarily due to this intrinsic difficulty, the state of the art for dynamic object reconstruction falls far behind that of static scene reconstruction.

The problem of dynamic object reconstruction is fundamentally under-constrained and requires further assumptions. Many existing works make various assumptions on scene geometry, object motion, capture pattern, \emph{etc}. For instance, most non-rigid structure from motion (NRSFM) methods assume the 3D shapes of deforming objects lie in a low dimensional subspace, and hence any shape can be represented as a linear combination of $K$ shape bases \cite{Bregler_CVPR2000,torresani2008nonrigid,dai2014simple}. Trajectory-based methods assume smooth motion of the dynamic objects across time \cite{Akhter_NIPS08}. \citet{bao2011toward} assume a non-coplanar layout of objects and cameras, plus a prior knowledge of the object size. \citet{Hoiem_CGRAPH2005} assume the scene only consists of the elements of sky, ground plane, and vertical buildings facades. When developing methods for dynamic object reconstruction, in addition to making valid assumptions, having fewer but more general assumptions is vital to enable the methods to work more universally and robustly in real scenarios. 

Trajectory triangulation -- one kind of dynamic object reconsturction problems -- computes the trajectory of a dynamic 3D point given a set of unitary observations across time. Under the assumption of smooth object motion and available sequencing information (\ie~the temporal order of images being taken), existing method can achieve accurate reconstruction results \cite{Park_ECCV2010,Valmadre_CVPR2012}. Although the assumption of smooth object motion is typically true for real dynamic objects, in practice easily obtaining the sequencing information and achieving high reconstruction accuracy cannot be satisfied simultaneously \cite{ZhuCL_CVPR11,Valmadre_CVPR2012}. The sequencing information tells that a moving 3D point is spatially close, if the temporal stamp of two capturing images are close. In effect, it is this spatial proximity that is leveraged by the existing methods \cite{Park_ECCV2010,Valmadre_CVPR2012} for reconstruction. In contrast, our research focuses on 3D reconstruction of dynamic objects given no or partial information of the spatial/temporal proximity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Thesis Statement}
Robust and efficient depthmap estimation for Internet collected photos is attained through a framework of joint view selection and depthmap estimation, and accurate dynamic object reconstruction from unstructured images or unsynchronized videos is achieved given no or partial information of spatial/temporal proximity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outline of Contributions}
This dissertation contributes significantly to advance the state-of-the-art techniques for the problems of static scene reconstruction and dynamic object reconstruction, and it builds on our published works \cite{zheng2014patchmatch,zheng2014joint,zhengiccv_2015}.

\begin{description}

\item[PatchMatch Based Joint View Selection and Depth Estimation:]
Chapter \ref{ch:patchmatch} focuses on the problem of depthmap estimation using Internet collected photos. The non-controlled input imagery presents practical challenges such as heterogeneous scene illuminations and unstructured viewing geometry. Therefore, it is vital to determine a subset of images or pixels in the dataset for robust depth estimation. Moreover, the ever-increasing number of crowd-sourced datasets have explicitly brought efficiency and scalability to the forefront of application requirements.

To solve this problem, we propose a probabilistic framework for joint view selection and depth estimation at the pixel level. Our new method attains more complete depthmaps compared to the state-of-the-art method for Internet collected photos \cite{Goesele07}. To increase the efficiency and scalability, our framework seamlessly incorporates the PatchMatch scheme \cite{patchMatchStereo1} to  reduce the size of the depth hypotheses set. Also, the memory requirement of our framework scales linearly with respect to the number of source images, as opposed to exponentially \cite{CombinedDepthOutlier}. Moreover, our method is designed to process each row or column of the reference image independently, enabling easy parallelization and GPU implementation.

\item[Joint Object Class Sequencing and Trajectory Triangulation:]
Chapter \ref{ch:jost} targets the problem of reconstructing the 3D positions of dynamic objects from a set of unstructured images. Each dynamic object is observed by the images once so that the traditional 3D triangulation for static scenes is impossible. To tackle the fundamentally under-constrained problem, we assume that all of the objects of the same class (\eg~pedestrians or cars) move in a common path in 3D space. Then, our method estimates the 3D positions of the dynamic objects by triangulating the trajectory formed by all the objects moving in the common path. 

To the best of our knowledge, no current methods have solved this challenging problem. 
Our method uses the object detection outputs as a general feature for each dynamic object, as opposed to typical image features such as points or edges. In solving the problem, recovering the sequencing information, which is defined as the topology of the trajectory in this specific problem, is vital for trajectory triangulation. We propose to jointly estimate the sequencing information and the 3D points, which is posed as minimizing a nonconvex function. To minimize the function, we propose a novel discrete-continuous optimization approach based on the generalized minimum spanning tree (GMST). 

\item[Dynamic Object Reconstruction from Unsynchronized Videos:]
Chapter \ref{ch:video_l1} aims at the similar problem of dynamic object reconstruction, but using unsynchronized video streams as input. To handle this unconstrained problem, we observe that any shape at one time instance is a linear combination of other shapes (self-expression), under the assumption of smooth object motion. The problem is then solved by learning a self-expressive dictionary, which is defined as a collection of temporally varied structures. 

The main contribution of this paper is solving the new problem of dynamic object reconstruction without temporal order information across video streams (also called sequencing information). This is contradictory to the existing works that strictly rely on available sequencing information \cite{Park_ECCV2010,Valmadre_CVPR2012}. 
Moreover, to the best of our knowledge, we are the first to use the self-expression prior for dynamic object reconstruction. This prior has the potential to be used in the traditional non-rigid structure from motion (NRSFM) problem, where most existing methods use the assumption that any shape is a linear combination of $K$ fixed shape bases \cite{dai2014simple,Bregler_CVPR2000}. In learning the dictionary, we propose an efficient solver based on the alternating direction method of multipliers (ADMM) \cite{boyd2011distributed}.

\end{description}

Each of these contributions addresses the issue of 3D reconstruction from 2D imagery. The next three chapters describe each method in detail, and Chapter \ref{ch:discussion} concludes the dissertation with potential extensions to our works and possible future research directions. 

