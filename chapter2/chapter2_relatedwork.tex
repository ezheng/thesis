%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Related work}

3D reconstruction from 2D imagery has been studied intensively and extensively by many researchers in the computer vision community. In this section, I first review the works for camera parameter estimation, and then survey the works related to static and dynamic object reconstruction respectively.

\section{Camera parameter estimation}
Camera parameters are generally considered as an prerequisite for 3D reconstruction, since they provide the geometric relationship among different cameras. For instance, with this geometric information, the mapping from a 3D point to an image pixel can be uniquely determined.
Being as two parts of the camera parameters, the internal (intrinsic) part has focal length, principle point, skew parameter, and radial distortion that convert the space coordinates to image coordiantes, the external (extrinsic) part describes a camera's rotation and translation relative to a global coordinate system \cite{Hartley2004}. 

Given the importance of camera parameters in computer vision tasks such as 3D reconstruction, lots of works have been done for camera parameters estimation, a process also called camera calibration. Earlier works for camera calibration require a calibration object such as a planar checkerboard to be seen by the cameras \cite{conf/cvpr/SturmM99,zhang2000flexible,caltoolbox}, which impose a significant constraint in its application. Thanks to the recent development of techniques in structure from motion (SfM) \cite{Snavely2,snavely2008modeling,WuVSFM,wilson2013network,heinly2014_duplicate_structure,schoenberger2015paige,Heinly,heinly_dissertation,zheng2015_structureless_resection}, camera calibration can be achieved by just leveraging 2D correspondences among multiple images. 

Structure from motion is a pipeline that targets estimating the cameras parameters of the images that share the same observations of a static scene. A typical pipeline includes the main steps of feature extraction \cite{lowe2004_sift,rublee2011_orb,bay2008_surf}, inlier correspondence searching \cite{raguram2013usac}, camera pose estimation \cite{nister2003_five_point,kneip2011novel,zheng2014general,zheng2015_structureless_resection} and bundle adjustment \cite{agarwal2010_ba,wu2011_multicore_ba}. Recent works  in structure from motion has exhibited its accuracy, efficiency and robustness to be applicable in most real scenarios.


%While structure from motion estimates camera parameters and produce a sparse 3D point cloud of a static scene, it is the task of stereo that builds a dense model.

\section{Static scene reconstruction}
As a main research subject in 3D computer vision, there are a ton of works addressing issues in static scene reconstruction. Early works mainly focus on depthmap estimation on binocular images \cite{boykov2001fast,Sun_ECCV2002_stereoBeliefProp,scharstein2002taxonomy,scharstein2007learning}. The two images are typically rectified so that searching the correspondence of a pixel is executed on the same line in another image. In contrast, multiview depthmap estimation (MVDE) use multiple images to reduce the ambiguity when searching correspondences. Moreover, the redundant information in the estimated multiple depthmaps can be leveraged to filter out wrongly estimated depth. %This step, called depthmap fusion, is also referred as a process to fuse depthmaps into a unified mesh model. 
This section first discusses the most related works for multiview depthmap estimation and the associated issues such as the robustness and the efficiency, and then discusses briefly the methods generating a consistent point cloud or mesh.

\subsection{Multiview depthmap estimation}
Handling occlusion is important in depthmap estimation, and it firstly emerged in two view stereo \cite{Sun_ECCV2002_stereoBeliefProp, Sun_CVPR2005_stereo, Xiao:2008}. However, in these methods, the occluded pixel region is only marked with unknown depth due to the unavailable correspondence in another image. 

In principle, the additional view  redundancy available to MVDE can be leveraged to resolve occlusions. \citet{handle_occlusion2001} explicitly address occlusion in multi-baseline stereo by only using the subset of the heuristically selected overlapping cameras with the minimum matching cost. The heuristic provides occlusion robustness as long as there is a sufficient number of unoccluded views (typically 50\%). \citet{MultiHypothesis_ECCV2008} choose the best few depth hypotheses for each pixel, following with a MRF optimization to determine a spatially consistent depthmap. Their method chooses source images based on spatial proximity of cameras. \citet{Strecha_BayesModelCVPR2004} handle occlusion in wide-baseline multi-view stereo by including visibility within a probabilistic model, where the depth smoothness is enforced on neighboring pixels according to the color gradient. The work of \citet{Strecha_BayesModelCVPR2004} is further extended in \citet{CombinedDepthOutlier} where the depth and visibility are jointly modeled by hidden Markov random fields. In \citet{CombinedDepthOutlier}, the memory used for visibility configuration of each pixel is $2^K$, which grows exponentially with respect to the number of input images $K$. Hence, the approach is limited to very few images (three images in their evaluation). In contrast, our memory usage is linear with the number of images $K$. %Comparing to above methods, our approach is far more scalable in terms of its source image capacity. This scalability is essential when dealing with large datasets such as internet photo collections.
\citet{Gallup08} present a variable-baseline and variable-resolution framework for MVDE, exploring the attainment of pixel-specific data associations for capture from approximately linear camera paths. While that work illustrates the benefits of fine grain data association strategies in multi-view stereo, it does not easily generalize to irregularly captured datasets. %Internet photo collection as targeted by our proposed method.

Given the redundant information among multiple depthmaps,
lightweight depthmap fusion removes outlier depth by leveraging the mutual depth consistency among multiple depthmaps. 
\citet{Shen_TIP2013} computes the depthmap for each image using PatchMatch \mbox{stereo}, and enforces depth consistency over neighboring views. \citet{LeastCommitment_3DIMPVT2012} follows a scheme similar to Camppbell \citet{MultiHypothesis_ECCV2008} but select the final depth through a process enforcing mutual consistency across all depthmaps. These methods require the depthmaps of other views to be available, while in contrast we aim at developing a method that directly outputs an accurate depthmap. 

%

\subsection{Robustness}
Robust stereo performance for crowd sourced data is an ongoing research effort.
%Due to the irregular spatial distribution and illumination aberration for Internet photo collections in general, a small set of images may not be sufficient for depth estimation, and determining data associations for depth estimation is a more difficult task.
%\citet{Frahm2010} discern a suitable input datum by using appearance clustering using a color augmented GIST descriptor \cite{oliva2001modeling} along with feature-based geometric verification.
%\todo{cite jared}
Images downloaded from Internet (such as Flickr\footnote{https://www.flickr.com/} or Panoramio\footnote{http://www.panoramio.com/}) by searching key words are unstructured imagery with a large portion of unrelated images. To discern a suitable input datum for stereo, \citet{Frahm2010}  use appearance clustering of a color augmented GIST descriptor \cite{oliva2001modeling} along with feature-based geometric verification. In contrast, \cite{Heinly} discover the image relationship by proposing a streaming paradigm that register images to a vocabulary tree built online. However, even the unrelated images are purged, using the data for stereo is still challenging due to the heterogeneous capture characteristics.

To estimate the depthmap of a image, \citet{Frahm2010} select the most related images based on the number of sparse feature points shared in common. The depthmap is estimated using the heuristic K-best planesweeping algorithm proposed by \cite{handle_occlusion2001}. Due to the issues such as illumination difference and occlusion, their estimated depthmaps are of low quality. 
\citet{InternetScaleMVS_CVPR2010} use structure from motion (SFM) to purge redundant imagery but retain high resolution geometry. Their iterative clustering merges sparse 3D points and cameras based on visibility analysis. Although intra-cluster image partitioning is not performed, the cluster size is limited in an effort to maintain computational efficiency. \citet{Goesele07} address the viewpoint selection for crowd sourced imagery by building small size image clusters using the cardinality of the set of common features among viewpoints and a parallax-based metric. This image-wide selection may not be robust to outlier pose estimates. After this, images are resized to the lowest common resolution in the cluster. Pixel depth is then computed using four images selected from the cluster based on local color consistency.

\subsection{Efficiency}
Efficiency is an important issue in depthmap estimation. Traditional methods on large baseline stereo generally involve exhaustive evaluations of a large number of depth hypotheses. The high complexity of computation is not only time-consuming \cite{yang2003multi,CombinedDepthOutlier,Gallup07,LeastCommitment_3DIMPVT2012}, but also prohibitive in less powerful devices such as smart phones. 

The recently proposed PatchMatch is an efficient sampling scheme. Though the scheme has no strict theory or proof of its working mechanism, it has been empirically shown that it always works very well in practice. The PatchMatch was firstly proposed to find approximate nearest neighbor matches between image patches in \citet{Barnes:2009:PAR}, and later \citet{patchMatchStereo1} introduce it to solve the two view stereo problem. PatchMatch initializes each pixel with a random slanted plane at random depth, and is followed by the propagations. The nearby and the current pixels' slanted planes are tested and the one with the best cost is kept. \citet{patchMatchStereo2} combine the PatchMatch sampling scheme and belief propagation to infer an MRF model that has smoothness constraints.
%By combining guided filter and PatchMatch, Lu \etal \cite{PMF_Hongsheng} provide an efficient edge-aware filtering for correspondence field estimation, which can be applied in two view stereo.
While the original PatchMatch stereo was a sequential method, \citet{patchMatchParallel} parallelize the algorithm by restricting the propagations to only horizontal and vertical directions. Our research further explore the potential of the PatchMatch in wide baseline stereo with large hypotheses space.

\subsection{Point cloud and mesh generation}
By far, we only discuss the works focusing on depthmap estimation. Some other methods aim at generating a consistent 3D model (either point cloud or mesh) instead of depthmaps. 
\citet{FURUKAWA_PAMI2010} aim at densifying the sparse point cloud from feature triangulation.
They present an accurate Patch-based MVS approach that starts from a sparse set of matched keypoints, which were repeatedly expanded until visibility constraints are invoked to filter out false matches. 
\citet{Zaharescu_PAMI2011} propose a mesh evolution framework based on a new self-intersection removal algorithm. 

A typical way for 3D mesh generation is to fuse the depthmaps into a consistent model by leveraging the redundant information across images. \citet{gallup20103d,gallup2010heightmap} develop heightmap-based fusion methods that work well for planar object surfaces such as building facades. \citet{zach2008fast} tackles the surface reconstruction task in a variational formulation.
Given that all these methods are volumetric-based and memory inefficient, \citet{zheng2012efficient} instead propose to compress the volume of interest by Haar wavelet, and hence much less memory is required. \citet{JAN} propose a method that reconstructs surfaces that do not have direct support in the input 3D points by exploiting visibility in 3D meshes. Their method has been shown to work pretty robustly on the regions with less texture.


\section{Dynamic object reconstruction}
The following sections outline the related works of trajectory triangulation, image sequencing, non-rigid structure from motion (NRSFM), and single-view reconstruction.

\subsection{Trajectory triangulation}
%Trajectory triangulation estimates the 3D points of dynamic objects given a sequence of images\cite{avidan2000trajectory,Park_ECCV2010,Valmadre_CVPR2012,ZhuCL_CVPR11,park20153d}.
\citet{avidan2000trajectory} first coined the task of trajectory triangulation, which is defined as  reconstruction of a moving point from monocular images. That is, each dynamic point is observed by only one camera at a time. Their method assumes the dynamic point moves along a simple parametric trajectory, such as a straight line or a conic section. Apparently, this is a rather strict constraint to impede its application in real scenarios. 
In contrast, some other methods  \cite{Park_ECCV2010,Valmadre_CVPR2012,ZhuCL_CVPR11,park20153d} focus on a more general model by only assuming a smooth motion of dynamic objects.

\citet{Park_ECCV2010} represent the trajectory with a linear combination of low-order discrete cosine transform (DCT) bases, and the trajectory is triangulated by estimating the coefficients of the linear combination.
There are two fundamental limitations of their method as observed in  \citet{Valmadre_CVPR2012}.
First, there is no automated scheme to determine the optimal number ($K$) of DCT bases.
Second, the correlation between the object trajectory and the camera motion inherently limits the reconstruction accuracy.
To overcome the first limitation, \citet{park20153d} select $K$ by checking the consistency of the reconstructed trajectory in an N-cross validation scheme.
Alternatively, \citet{Valmadre_CVPR2012} propose a new method without using DCT bases. They estimate the trajectory by minimizing the trajectory's response to a bank of high-pass filters.
To overcome the second limitation,
\citet{ZhuCL_CVPR11} propose to incorporate the 3D structures of a number of key frames to enhance the reconstructability. However, obtaining those key-frame 3D structures requires manual interaction. 
All the methods \cite{Park_ECCV2010,Valmadre_CVPR2012,ZhuCL_CVPR11} require the sequencing information of the images, 
but in natural capture setups, the availability of sequencing information and high reconstructability typically cannot be fulfilled simultaneously \cite{ZhuCL_CVPR11,park20153d}. 

%Zheng \etal~\cite{zheng2014joint} address a slightly different problem. They triangulate the object class trajectory, which is defined by the connection of the objects of the same class moving in a common 3D path, from a collection of unordered images. Their method jointly estimates the trajectory and sequencing, but has low scalability and efficiency due to the NP-hard GMST problem. In contrast, our proposed method reconstructs the dynamic objects without sequencing information across videos.

%Recently, Vicente \etal \cite{vicente2014reconstructing} introduced a dense reconstruction approach from unordered image sets leveraging manual object segmentation and semantic labeling for objects contained in the PASCAL-VOC dataset. Their method relies on the fact that similar objects of the same class in a similar pose are visible within the dataset. In contrast, our proposed method reconstructs the object shape without requiring the presence of similar object shapes in the dataset or leveraging semantic information to enable the reconstruction. Russell \etal \cite{russell2014video} introduce a monocular video based reconstruction method, which relies on hierarchical segmentation of a scene into objects, object parts, and background, in order to successfully reconstruct the scene. This approach assumes a known temporal ordering of the video frames and only considers a single camera's information. Our proposed method does not require known temporal ordering and leverages all available observations.

\subsection{Sequencing and Synchronization}
Sequencing information is important in trajectory triangulation. Recently,
\citet{Basha_ECCV2012, Basha_ICCV2013} target the problem of determining the temporal order of a collection of photos without recovering the 3D structure of the dynamic scene. The method in \citet{Basha_ECCV2012} relies on two images taken from roughly the same location to eliminate the uncertainty in the sequencing. \citet{Basha_ICCV2013} later introduce a solution that leverages the known temporal order of the images within each camera. Both of these methods assume dynamic objects move closely to a straight line within a short time period, but in practice, points can deviate considerably from the linear motion model, especially when the temporal discrepancy between images is large. 

Video synchronization has attracted much attention in the computer vision community \cite{Tuytelaars_CVPR,shrestha2010synchronization,rao2003view}. Those methods have various constraints such as camera motion, availability of sound, and number of videos. 
%While our approach aims at dynamic 3D reconstruction without sequencing, the local temporal order can be recovered as a byproduct of our approach. 

\subsection{Articulated object reconstruction}
%As another class of reconstruction, 3D articulated object reconstruction given monocular image sequences has received much attention. 
%Several particle filter %approaches have been developed for 3D human tracking [27, 26]. Wei et al. [32]
%and Valmadre et al. [31] reconstruct the 3D human poses from a small number of2D point correspondences obtained from uncalibrated monocular images. 
Trajectory triangulation suffers from the reconstructability problem of inaccurate reconstruction if the camera motion is relatively smaller than the object motion \cite{park20153d}. 
In the case of 3D reconstruction of articulated objects, we can enforce an additional constraint that the distances between joint points (according to the topology) are fixed, which helps to reduce ambiguities in reconstruction. 
Based on the previous work by \citet{Park_ECCV2010}, the authors further reconstruct 3D articulated motion with the constraint that a trajectory remains at a fixed distance with respect to a parent trajectory \cite{Park_ICCV2011}. Their work shows the improvement of the reconstructibility over their earlier approach \cite{Park_ECCV2010}.
However, the formulation involves solving an NP-hard quadratic programming problem, which is intractable in the case of a large number of input images. 
To conquer the limitation, \citet{Valmadre_ECCV2012} develop a dynamic programming approach that is guaranteed to solve the problem in a timely manner. 
As opposed to articulated object reconstruction, our research focuses on reconstructing more general dynamic objects. 

\subsection{Non-rigid SfM}
One class of related works solve the non-rigid structure from motion (NRSFM) problem, which targets simultaneous recovery of camera motion and 3D structure using an image sequence. These methods typically start from a set of 2D correspondences across frames. 
As an important extension of the well-known Tomasi-Kanade factorization \cite{tomasi1992shape}, 
the work by \citet{Bregler_CVPR2000} tackles the NRSFM problem through matrix factorization, with the assumption that deforming non-rigid objects can be represented by a linear combination of low-order shape bases. 
%It was later shown that in order to achieve a unique solution, more than just the orthogonality constraints have to be used~\cite{Xiao_ECCV2004}. To solve the ambiguity, prior knowledge is required to obtain a unique solution. 
It is later shown by \citet{Xiao_ECCV2004} that utilizing only orthogonality constraints on the camera rotation is not enough, and a basis prior is required to uniquely determine the shape bases.
%In the same spirit, various priors on the shape basis or shape coefficients have been introduced \cite{torresani2008nonrigid}.
However, \citet{akhter2009defense} discover that though the ambiguity in the shape bases is inherent, the 3D shape itself can be uniquely recovered without ambiguity.
Not until very recently, \cite{dai2014simple} propose a new prior-free method that estimates the shape matrix without explicitly recovering the shape bases, which is achieved by minimizing the nuclear norm of the shape matrix.

As a dual method to the above shape-based methods, \citet{Akhter_NIPS08} propose the first trajectory-based NRSFM approach, which leverages DCT bases to approximately represent object point trajectories.
While shape-based approaches typically do not require sequencing information,  trajectory-based approaches completely fail if image frames are randomly shuffled \cite{dai2014simple}.

At first glance, it seems that the NRSFM problem targets a more complete problem than that of trajectory triangulation since the former additionally assumes unknown camera poses as opposed to the latter.
However, these approaches assume orthographic or weak perspective camera models, and it has been shown empirically that the extension of these methods to the projective camera model is not straightforward \cite{Park_ECCV2010}. There are works for projective non-rigid shape and motion recovery based on tensor estimation \cite{hartley2008perspective,vidal2006nonrigid}, but this challenging problem is still under on-going research. Moreover, the NRSFM methods only recover the shape of the object without absolute translation due to the inherent ambiguity arising from the unknown shape translation and the unknown camera translation. 

%In contrast, our approach aims for solving the non-rigid structure from motion problem by leveraging unordered trajectories observed by perspective cameras.
%He \etal \cite{YuchaoDai_CVPR2012} proposed a prior-free solution to the non-rigid structure from motion problem for orthographic cameras which does not require any known temporal relations. However, in order to succeed, they require known ordered trajectories. In contrast, our proposed method does not require ordered trajectories.

%Another related class of methods are the approaches for articulated object reconstruction from monocular videos and images sequences.
%Urtasun \etal \cite{Urtasun_CVPR2006} use Gaussian Process Dynamical Models (GPDMs) to learn human pose and motion priors for 3D people tracking.
%With Bayesian model averaging, a GPDM can be learned from relatively small amounts of data, and it generalizes gracefully to motions outside the training set.
%A popular class of approaches are the particle filter based 3D tracking methods \cite{SminchisescuT_CVPR2003, Sidenbladh_ECCV2000}.

%Tuytelaars \etal~\cite{Tuytelaars_CVPR} propose a method to automatically synchronize two video sequences of the same event. They do not use any constraints from the scene or cameras, but rather rely on point correspondences among the video sequences.

%Unsupervised dictionary learning has also been widely used for reconstructing noisy signals and is closely related to our proposed framework. 
%Along these lines, Mairal \etal~\cite{Mairal_PAMI} show that better results can be obtained when the dictionary is tuned to the specific task.
%In~\cite{Mairal_NIPS} the authors further observe that using sparse coding to the problem of dictionary learning can reduce storage and computational requirements, as well as obviate the need of an explicit learning rate tuning.

%Tuytelaars \etal~\cite{Tuytelaars_CVPR} propose a method to automatically synchronize two video sequences of the same event. They do not use any constraints from the scene or cameras, but rather rely on point correspondences among the video sequences. Wedge \etal \cite{wedge2005_cvpr} propose an efficient coarse-to-fine approach for synchronizing two video sequences recorded at the same frame rate by stationary cameras with fixed internal parameters. In contrast to these methods, we propose a framework for the temporal alignment of multiple video sequences with arbitrary capture characteristics. After briefly discussing related work, we now proceed to provide the notation and intuitions behind our proposed method.


\subsection{Single image reconstruction}
While trajectory triangulation and NRSFM estimate 3D points from an image sequence, some other works target the problem of 3D reconstruction from a single image. Since there is only one shot of the object, the object motion, either static or dynamic, becomes irreverent for the reconstruction. 

Some works focus on 3D reconstruction of Manhattan world \cite{Coughlan_ICCV99}, which is defined as
man-made scenes with mainly orthogonal facades. In this scenario, 3D reconstruction from a single image can be simplified to finding 3D lines and planes within the scene. The work by \citet{Delage_ISRR2005} uses a MRF model to identify the different planes and edges in the scene, as well as their orientations. Then, an iterative optimization algorithm is applied to infer the planes' positions. \citet{Ram_ICCV2013} reconstruct the 3D lines in a Manhattan scene from a single image using linear programming that identifies a sufficient minimal set of least-violated line connectivity constraints.

There are other approaches mainly relying on supervised learning. 
\citet{Hoiem_CGRAPH2005} label the image regions as ground, vertical, and sky with a pre-trained classifier, and then ``cut and fold" the image into a pop-up model like children's pop-up book. The method is limited to the application of outdoor scenes containing simple ground and vertical structures.
\citet{Saxena_IJCV2008} propose a method for reconstruction from a single image. They compute reasonable depthmaps from a single still image by using a hierarchical multi-scale Markov Random Field (MRF) that incorporates several features. The parameters of the MRF model are trained using ground truth depth.  Recently, \citet{eigen2014depth} estimate the depthmap of a single image by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. Currently, single depthmap estimation using learning is still an active research topic.
%While most approaches for single image depthmap estimation only recovers relative depth, this dissertation focuses on  


















