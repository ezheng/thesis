\chapter{Joint Object Class Sequencing and Trajectory Triangulation (JOST)} \label{ch:jost}

\section{Introduction}

Techniques of 3D reconstruction from crowd-sourced imagery have developed rapidly over the past decades \cite{agarwal2011building,Frahm2010,zheng2014patchmatch,Heinly}. 
Despite these advances, the state-of-the-art methods only target the static parts of a scene, treating the dynamic elements as hindrances to reconstruction. Since dynamic objects are typically the major focus of real-life images, recovering their 3D information enables applications such as better scene visualizations and dynamic event analysis. Therefore, it is of great interest to reconstruct these dynamic objects.

\begin{figure}
\centering
\subfloat[]{
\centering
    \includegraphics[height=0.33\linewidth]{chapter4/resource/images_cropped.pdf}
    \label{fig:franklin_imgs}
     \label{fig:franklin_sfm}
    }
\subfloat[]{
\centering
    \includegraphics[height=0.33\linewidth]{chapter4/resource/3DModel.jpg}
    \label{fig:FranklinSingleViewRecon}
    }
\caption[Example input and output of JOST.]{Left: Tree images of the pedestrian dataset and the output of structure from motion. Right: Estimated 3D positions of two pedestrians that are captured in the image. Note we only reconstruct one 3D position for each dynamic object instance instead of a dense 3D model. For visualization purposes, a general mesh model is inserted into each estimated position.}
%\end{subfigure}
%\label{fig:gmst}
\vspace*{-0.5cm}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.65\linewidth]{chapter4/resource/path_cropped.pdf}
\caption[Example of cross-shaped object class trajectory.]{Example of cross-shaped object class trajectory. The circles of different colors represent object instances of the same object class in the path. Note the topology of the trajectory is tree-structured. Each image only observes one or a few object instances, and we use all the observations to recover the object class trajectory.}
\label{fig:oct_example}
\end{figure}

In this work, we propose a method to estimate the 3D positions of dynamic objects of the same class moving in a common path given a set of unstructured images as input. 
Figure \ref{fig:franklin_imgs} shows example input images in a dateset that captures  pedestrians walking on a sidewalk. We assume no temporal correlation among the images, and that no two images observe the same dynamic object instance. 
The main challenge of the reconstruction problem resides in recovering 3D positions given  non-current captures (or even single observations) of the dynamic objects, which invalidates the use of traditional 3D triangulation. The only constraint available for our problem is the fact that all observed instances of an object class move along a possibly diverging path in the 3D scene, which we define as an object class trajectory.
Figure \ref{fig:oct_example} shows one example of object class trajectory.

In this chapter, we define the spatial ordering of the objects along the trajectory as sequencing information. If the trajectory is modeled as a graph, this information can also be regarded as the topology of the trajectory. This sequencing information captures the spatial proximity of the dynamic objects in 3D space, and therefore triangulating the object class trajectory necessarily involves learning a trajectory topology.
To recover the object class trajectory, our method simultaneously determines the sequencing information of the objects and their 3D positions on the path, which we call joint object class sequencing and trajectory triangulation (JOST). We leverage all the observations on different images to recover the object class trajectory, which in turn provides an estimate for the 3D positions of the dynamic objects in each image (see Figure \ref{fig:FranklinSingleViewRecon}).



\section{\JOST}

We now detail our method for joint object class sequencing and trajectory triangulation from unstructured images. 
%, which in particular removes the constraint of known temporal camera information and known object positions.
Our method includes three steps:
\begin{enumerate}
\item \label{challenges_sfm} Spatially register the cameras to a common 3D coordinate system using structure from motion (SfM).
\item \label{challenges_tang} Detect object instances and estimate motion tangents from input imagery as the 2D observations of the dynamic objects.
\item \label{challenges_order} Leverage the observations of the object instances to simultaneously
\begin{enumerate}
\item determine the sequencing information of the objects along a trajectory (\emph{i.e}. the topology of the trajectory), and
\item triangulate the geometry of the corresponding object class trajectory.
\end{enumerate}

\end{enumerate}
While we exploit known methods to solve for camera registration, object detection, and motion tangents in the images, our main contribution is an algorithm for tackling challenge  \ref{challenges_order}. To this end, we model our problem as a nonconvex optimization problem, and develop a novel solver involving a step of discrete optimization followed by another step of continuous refinement. 
Next, we introduce our system in detail.

\subsection{Spatial Registration}
The goal of the initial spatial registration in our method is to establish camera registration in a common coordinate system.
Given that in all our datasets a fair portion of the images contains static background structures, we use the publicly available structure from motion tool VisualSFM  \cite{WuVSFM} to register all the cameras. See Figure \ref{fig:franklin_sfm} for an example.

The obtained camera registration determines the camera center $\mathbf{\tilde{C}}_j$ of the $j$-th camera. 
With known camera parameters, each pixel in a camera defines a viewing ray with direction $\mathbf{r}$ in the 3D scene space. For our object class trajectory, we are only interested in the ray direction $\mathbf r_i$ associated with the object instance $i$ of the desired class (for simplicity we refer to them as objects), where  $i =1, \dots, N$, and $N$ is the total number of detected objects over all frames. %Hence, we only aim to compute the ray direction $\mathbf{r}_i$ for pixels belonging to  the detected object $i$. 
The ray $\mathbf{X}_i(t_i)$ in the 3D space represents a 1D subspace on which the imaged object has to lie and is described by
\begin{equation}
\label{eq:ray}
\mathbf{X}_i(t_i)=\mathbf{C}_i + t_i \mathbf{r}_i,
\end{equation}
where $t_i \geq 0$ is the positive distance from the camera center $\mathbf{C}_i$ along the ray $\mathbf{X}_i(t_i)$.
In the following, we implicitly assume the condition $t_i \geq 0$.
We denote the camera center associated with an object instance $i$ as $\mathbf{C}_i$  with $\mathbf{C}_i =\mathbf{\tilde{C}}_j$, where $\mathbf{\tilde{C}}_j$ is the center of the camera $j$ in which the object instance $i$ is detected. This means if more than one object is detected in camera $j$, there will be multiple $\mathbf{C}_i$ with identical positions. Once we obtain the value for $t_i$, the object position can be uniquely determined. 
%The unknown true distance of the object instance $i$ along ray $\mathbf{X}_i$ is denoted as $\hat{t}_i$. Once obtained, it determines the 3D object position $\mathbf{\hat{X}}_i$. %Next we turn to detail our object class instance detection and the motion tangent estimation.


\subsection{Object Detection and Motion Tangent Estimation}
\label{sec:detection}

Our proposed method leverages object detection techniques to determine the 2D observations of the dynamic objects. We identify one 2D position of each detected object  on the image by the center of the detection bounding box. These object detections provide us the viewing rays where the dynamic objects are placed. 

To robustly perform joint object class sequencing and trajectory triangulation, our proposed method also uses the motion tangent of each object, which is defined as the moving direction of the dynamic object in the 3D space. 
The problem of motion tangent estimation has been solved for videos \cite{zhao2003face}, but  in the absence of temporal coherence among the images, our method need to estimate the motion tangent based on a single image.

The particular choice of object detection and motion tangent estimation methods depends on the specific object class and the scenes. We discuss our choices in Section \ref{sec:face_detection}, and for now we assume we have at our disposal the 2D observation  defining the ray $\mathbf{X}_i(t_i)$, as well as a coarse estimate of the motion tangent $\mathbf{d}_{i}$ for each object $i$.

\subsection{Object Class Trajectory Triangulation}
\label{sec:problem}
Assuming known viewing rays $\mathbf{X}_i(t_i)$ and the motion tangent $\mathbf{d}_{i}$, we now define the object class trajectory estimation problem before delving into our data representation and our estimation framework. For ease of description, we directly leverage the viewing rays $\mathbf{X}_i(t_i)$ of the detected objects $i$ and thereby implicitly use the camera parameters and the 2D observations.

For a particular class of objects, an object class trajectory describes a path taken by the dynamic objects of the desired class through the 3D scene. Since we only have a finite number of observations of objects along the path, we only sample a discrete set of 3D points on the path,
and the combination of piecewise linear functions between the true object positions $\mathbf{{X}}_i^*$  represents the object class trajectory.
%, but in general any parametric representation of the path can be chosen.
%The desired object class trajectory is the path of minimal length and it can be determined through a minimization of the cost function

An important principle for obtaining an object class trajectory is that sampling along a path results in a collection of spatially adjacent points. A trajectory should therefore connect all observed points in such a way that total (spatial) traversal between the points is minimized. We formulate this as a minimization of the following cost function:

\begin{equation}
\label{eq:pathsimple}
\underset{\mathbf{p}} { \min  }\sum_{(i,j)\in\mathbf{p}}{\|\mathbf{X}^*_i-\mathbf{{X}}_j^*\|_2^2}.
\end{equation}
here $\mathbf{p}$ defines the topology spanning the path with minimum cost, given as a list of adjacency relationships between all the points $\mathbf{X}_i^*$, $i=1, \dots, N$. 

While the trajectory above is based on the ground truth 3D object positions $\mathbf{X}_i^*$, we can only observe the rays $\mathbf{X}_i(t_i)$. To recover the object class trajectory, we also need to determine the position of each object $i$ along its viewing ray $\mathbf{X}_i(t_i)$.
We propose to find the adjacency relation by optimizing over variables $\mathbf{t}=[ t_1, \dots, t_N ]$ and $\mathbf{p}$ jointly as
\begin{equation}
\label{eq:pathsimpleray}
\underset{\mathbf{p},\mathbf{t}} { \text{min} }\sum_{(i,j)\in\mathbf{p}}{\|\mathbf{X}_i(t_i)-\mathbf{X}_j(t_j)\|_2^2}. \\
\end{equation}
To robustly recover both $\mathbf{t}$ and $\mathbf{p}$ simultaneously, we further leverage the information of motion tangent. The direction of the local trajectory should be the same or similar to the motion tangent of the dynamic objects. Given the motion tangents $\mathbf d_i$ estimated from the images, we can further constrain the trajectory, obtaining an optimization problem:
\begin{equation}
\label{equ:costFunc}
\underset{\mathbf{p},\mathbf{t}} { \text{min} }
\sum_{(i,j)\in\mathbf{p}}{\|\mathbf{d}_{i,j}\times(\mathbf{X}_i(t_i)-\mathbf{X}_j(t_j))\|_2^2 + \lambda\|\mathbf{X}_i(t_i)-\mathbf{X}_j(t_j)\|_2^2}, \\
%\nonumber
%\text{with } \mathbf{X_i}=\mathbf{C_i} + t_i \cdot \mathbf{r_i}  ,\quad t_i \geq 0, \quad i =1,2,...N
\end{equation}
where the operator $\times$ is the vector cross product, and $\lambda$ is a positive weight (discussed at length in Section \ref{sec:analysis}). The direction $\mathbf{d}_{i,j}$ is selected from $\mathbf{d}_i$ and $\mathbf{d}_j$ as the motion tangent that is closest to the 3D motion direction $\mathbf{X}_i(t_i)-\mathbf{X}_j(t_j)$.
The first cost term in Equation (\ref{equ:costFunc}) adds the penalization if the local direction of the recovered trajectory deviates from the motion tangent.
% In Equation (\ref{equ:costFunc}), the penalty of the first term increases if the direction of the vector $\mathbf{X}_i(t_i)-\mathbf{X}_j(t_j)$ deviates from $\mathbf{d}_{i,j}$. 
The optimization procedure simultaneously determines both the adjacency $\mathbf{p}$ and the object positions through $\mathbf{t}$.

%Traditionally, these problems have been treated separately as either a sequencing problem, where the 3D points are given and only the sequence of traversal needs to be estimated, or as a trajectory triangulation problem \cite{Park_ECCV2010,Valmadre_CVPR2012}, where the sequence of observations for the trajectory is given and the 3D points along the trajectory need to be determined. 
%Our proposed method generalizes these problems into a common framework to allow for the simultaneous estimation of the adjacency of the trajectory and the 3D position of the observations. %After defining the cost function (Equation (\ref{equ:costFunc})) for the optimization problem to determine the object class trajectory,
Optimization of the non-convex function in Equation (\ref{equ:costFunc}) is inherently difficult. To achieve this, we propose a new discrete-continuous  optimization strategy using a generalized minimum spanning tree (GMST).

\subsection{Generalized Trajectory Graph}
\begin{figure}[t]
\centering
\subfloat[Discretization of viewing ray]{
   \includegraphics[height=0.33\columnwidth]{chapter4/resource/1_pdfsam_gmst_eccv_cropped.pdf}
   \label{fig:discrete_pts}
}
\subfloat[Multi-partite graph instance]{
   \includegraphics[height=0.33\columnwidth]{chapter4/resource/2_pdfsam_gmst_eccv_cropped.pdf}
   \label{fig:multi_partite_graph}
}
\subfloat[Estimated 3D trajectory]{
   \includegraphics[height=0.33\columnwidth]{chapter4/resource/3_pdfsam_gmst_eccv_cropped.pdf}
   \label{fig:estimated_trajectory}
} 
\caption[Illustration of the generalized minimum spanning tree (GMST)]{Illustration of GMST. See the text for more details.}
%\end{subfigure}
\label{fig:gmst}
\end{figure}

\label{sec:gmst}
To determine the object class trajectory, we conceptually have to choose for each ray $\mathbf{X}_i(t_i)$ the 3D point, and simultaneously determine the adjacency $\mathbf{p}$ representing the adjacency relations of the rays $\mathbf{X}_i(t_i)$, which defines the topology of the object class trajectory.
Our discrete-continuous optimization strategy first uses a generalized minimum spanning tree (GMST) to find the adjacency list $\mathbf{p}$, and followed by a convex optimization over $\mathbf{t}$ with $\mathbf{p}$ being fixed.

In the discrete optimization step, we map the continuous problem of finding the 3D point along each ray to a discrete problem of selecting a 3D point out of a set of discrete 3D points (see Figure \ref{fig:discrete_pts}). Using this formulation, we determine one 3D point along each ray and the adjacency $\mathbf{p}$ by computing the GMST on an undirected multipartite graph $\cal{G}(V,E)$~\cite{MyungLT_95}. This allows us to simultaneously  determine the topology and the discrete 3D object positions.

An undirected multipartite graph is a graph $\cal{G}(V,E)$ whose vertices are partitioned into $N$ partite sets $\left\{V_1, \dots, V_N\right\}$ with $\vert V_i \vert = k$, while fulfilling ${\cal V}=V_1 \cup V_2 \cup \dots \cup V_N$ and $V_o \cap V_p = \emptyset, \forall o\neq p$, with $o,p \in \{1, \dots, N \}$. The multipartite graph $\cal{G}(V,E)$ has only edges between the different partite sets of vertices $V_o$, and all edge costs are non-negative (see Figure \ref{fig:multi_partite_graph} for an example). Next, we will explain on how we define the graph $\cal{G}(V,E)$ based on Equation \ref{equ:costFunc}.

Each ray $\mathbf{X}_i(t_i)$ defines a one dimensional constraint on the 3D position of the object. We discretize the ray to obtain a discrete set of potential depth estimates. This leads to a finite set of possible 3D positions along the ray (see Figure \ref{fig:discrete_pts} for an illustration), defining a finite set of 3D point hypotheses 
$\{\mathbf{\hat{X}}_i^o~|~o=1,\dots,k \}$,
where $k$ is the number of the discrete hypotheses along the ray.
In our representation, each 3D point $\mathbf{\hat{X}}_i^o$ establishes a node $V_i^o$ in the graph. The set of nodes $\{V_i^o~|~o=1, \dots, k\}$ related to the ray $\mathbf{X}_i(t_i)$ of object $i$ defines a partite set of nodes $V_i$ in the graph $\cal{G}(V,E)$. 
Given that no nodes within a group have any connecting edges, the resulting multipartite graph will contain no edges between the different depth hypotheses of object $i$.
%it enforces the traversal of the graph to be constrained among partite sets. 
%This is  consistent with the understanding of moving the different instances of an object class in the scene to determine the object class trajectory.

We now define the edge cost of the multipartite graph based on Equation (\ref{equ:costFunc}).
The multipartite graph only has edges between the nodes from different partite sets.
%Given the properties of the multipartite graph we only insert edges between the nodes that are in different partite sets of nodes.
We define the edge direction $\mathbf{d}_{i,j}$ between any two nodes $V_i^o$ and $V_j^p$ in the partite set $i$ and partite set $j$, respectively, as the consistency of the 3D motion with the motion tangents $\mathbf d_i$ or $\mathbf d_j$ (see Section  \ref{sec:detection}). This definition comes from the intuition that the edge direction should be compliant with the motion tangent observed in the images. Given the motion of two objects $i$ and $j$, and their respective motion tangents $\mathbf d_i$ and $\mathbf d_j$, it is clear that the edge direction between the points $\mathbf{\hat{X}}_i^o$ and $\mathbf{\hat{X}}_j^p$ (associated with the nodes $V_i^o$ and $V_j^p$) should be close to at least one of the motion tangents $\mathbf d_i$ and $\mathbf d_j$.
Therefore, we define the edge cost $e(V_i^o,V_j^p)$ of the edge between the nodes $V_i^o$ and $V_j^p$ as

\begin{equation}
\label{eq:edgecost}
e(V_i^o,V_j^p)=\text{min}(\|\mathbf{d}_i\times(\mathbf{\hat{X}}_i^{o}-\mathbf{\hat{X}}_j^{p})\|_2^2, \|\mathbf{d}_j\times(\mathbf{\hat{X}}_i^{o}-\mathbf{\hat{X}}_j^{p})\|_2^2) +\lambda\|\mathbf{\hat{X}}_i^{o}-\mathbf{\hat{X}}_j^{p}\|_2^2.
\end{equation}
If only considering the first term in Equation (\ref{eq:edgecost}), edges with 3D motion directions that are approximately parallel to $\mathbf{d}_i$ or $\mathbf{d}_j$ have lower  cost than those are at an angle to both $\mathbf{d}_i$ and $\mathbf{d}_j$. For instance, %for the edge cost from Equation (\ref{eq:edgecost}),
Edge 1 and Edge 3 in Figure \ref{fig:node_DirectionB} have a relatively lower cost than Edge 2 because Edge 1 is parallel to $\mathbf{d}_j$ and Edge 3 is parallel to $\mathbf{d}_i$.

\subsection{GMST}
\begin{figure}[t]
\centering
\subfloat[]{
    \includegraphics[height=0.19\textheight]{chapter4/resource/nodeDirection1_cropped.pdf}
    \label{fig:node_DirectionA}
}
\subfloat[]{
    \includegraphics[height=0.19\textheight]{chapter4/resource/nodeDirection2_cropped.pdf}
    \label{fig:node_DirectionB}
}
\subfloat[]{
    \includegraphics[height=0.19\textheight]{chapter4/resource/edgeReduction_cropped.pdf}
    \label{fig:node_edgeRemove}
}

\caption[Illustration of the motion tangent.]{In Figure \ref{fig:node_DirectionA}, the black nodes shows the real positions of dynamic objects. The red vector represents the direction associated with each object. In the shown example, $\mathbf{d}_{i,j}$ equals $\mathbf{d}_{i}$.  }
%\end{subfigure}
\label{fig:nodeDirection}
\end{figure}

A generalized minimum spanning tree (GMST) on the graph $\cal{G}(V,E)$ is a tree of minimal cost that spans \emph{exactly one node} from each partite set $V_i$. The GMST problem degenerates to a typical minimum spanning tree problem \cite{Cormen:2001:IA:580470} if each of the partite sets contains only one node.
For our proposed graph, it means a GMST includes exactly one hypothesized 3D point from each observation. Furthermore, a GMST prefers the edge $e(V_i^o,V_j^p)$ that has a small cost and is compliant with the motion tangents in the images, as those edges have lower edge cost. Accordingly, a GMST is our desired solution for estimating the object class trajectory. Note that if we sample an infinite number of 3D points along each viewing ray, the corresponding GMST problem is equivalent to the original formulation in Equation (\ref{equ:costFunc}).% before discretization for the multipartite graph formulation.

The multipartite graph defined above contains a large number of edges, which increases the complexity of computing the GMST.
We use a deterministic method introduced by \citet{Ferreira_ESWA2012} to remove the redundant edges that are guaranteed not to be included in the GMST. We show a specific toy example in Figure \ref{fig:node_edgeRemove} to illustrate the method. If the cost of edge $(u,v)$ is larger than any cost of the 6 edges $(u,n_l)$ and $(v,n_l)$, $l=1,2,3$, the edge $(u,v)$ is safe to be removed. A simple proof is that if edge $(u,v)$ exists in the computed GMST, we could remove edge $(u,v)$ and replace it with one of the 6 edges to obtain a new GMST with lower cost. Therefore, edge $(u,v)$ can not be present in the GMST. Moreover, it is plausible to explore other ways to remove edges based on given prior information. For instance, if it is known the pairwise neighboring 3D objects are close in 3D space, we can safely remove the edges that connect two spatially distant point hypotheses by applying a predefined threshold.

The GMST problem was first introduced by \citet{MyungLT_95} and has been extensively studied in the past two decades \cite{MyungLT_95,Dror_EJOR,Feremans_LL02,Oncan_CL08,Ferreira_ESWA2012} due to its wide applications in  telecommunications, agriculture watering, and facility distribution design \cite{MyungLT_95,Dror_EJOR}. Unlike the minimum spanning tree (MST) problem, which can be solved in polynomial time, finding the GMST is proved to be NP-hard \cite{MyungLT_95}. Myung \etal \cite{MyungLT_95} and Feremans \etal \cite{Feremans_LL02} propose several integer programming formulations for the GMST problem. However, those methods provide no guarantee of efficiency, especially when the problem scale is large. The computational challenge of the GMST problem has led to the development of metaheuristics \cite{Oncan_CL08,Ferreira_ESWA2012} that search the hypothesis space and are empirically shown to be effective.

We exploit the state-of-the-art GRASP-based approach proposed by Ferreira \etal \cite{Ferreira_ESWA2012}. GRASP (Greedy Randomized Adaptive Search Procedure) is a metaheuristic that consists of iterations comprising two phases: 1) solution construction and 2) solution improvement through local search. \citet{Ferreira_ESWA2012} propose a method that considers several solution construction algorithms, a local search procedure, and two additional mechanisms: path-relinking and iterative local search. We refer readers to their paper \cite{Ferreira_ESWA2012} for more details.

\subsection{Continuous Refinement}
The output of GMST computation is the estimation of the 3D points (denoted as $\mathbf{\widehat{X}}_i$ for object $i$) and the adjacency topology $\mathbf{p}$ of the object class trajectory. Then, $\mathbf{d}_{i,j}$ is chosen to be one of $\mathbf{d}_i$ and $\mathbf{d}_j$  that has smaller angle to the vector $\mathbf{\hat{X}}_i - \mathbf{\hat{X}}_j$,
\begin{equation}
\mathbf{d}_{i,j}= \underset{\mathbf{d}\in{\{\mathbf{d}_i, \mathbf{d}_j}\} }{\arg\!\max}(|\mathbf{d} \cdot (\mathbf{\hat{X}}_i - \mathbf{\hat{X}}_j)| ),
%\mathbf{d_{i,j}} = \text
\end{equation}
where operator $\cdot$ is the vector dot product.
We fix the adjacency $\mathbf{p}$ given by the GMST and continue with a final continuous refinement step for the 3D object position, through a convex program optimization over variable $\mathbf{t}$
%To achieve this we fix the adjacency $p$ and only optimize over the object positions $\mathbf{t}$ in Equation (\ref{equ:costFunc}). This leads to the following optimization problem:
\begin{equation}
\underset{\mathbf{t}} { \text{min} }
\sum_{(i,j)\in{\mathbf{p}}}{\|\mathbf{d}_{i,j}\times(\mathbf{X}_i(t_i)-\mathbf{X}_j(t_j))\|_2^2 + \lambda\|\mathbf{X}_i(t_i)-\mathbf{X}_j(t_j)\|_2^2}.
\label{equ:costFunc_continuous}
\end{equation}

\subsection{Reconstructability Analysis} \label{sec:analysis}

Now, we analyze the reconstructability of the proposed method. That is, we determine under which conditions the solution of Equation (\ref{equ:costFunc}) generates accurate 3D points. The direct analysis of Eq (\ref{equ:costFunc}) is difficult, since it needs to determine in which situation the adjacency $\mathbf{p}$ with minimum cost, out of $N^{N-2}$ possible adjacencies \cite{wiki_cayley_alg}, corresponds to the real object class trajectory.
However, we find that having the motion tangent constraint reduces the possibility of finding the incorrect adjacency $\mathbf{p}$. 
Hence, we focus on  the reconstructability of the continuous method in Equation (\ref{equ:costFunc_continuous}) given the adjacency $\mathbf{p}$.

Assume we already know the ground truth 3D point $\mathbf{{X}}_i^*$ of object $i$, $i=1,\dots,N$. Given that $\mathbf{{X}}_i^*$ is present on the viewing ray $\mathbf{X}_i$, we move the camera center $\mathbf{C}_i$ to $\mathbf{X}_i^*$ along the ray $\mathbf{X}_i(t)$ in direction $\mathbf{r}_i$. Then any point on the line that passes through $\mathbf{X}_i^*$ and has ray direction $\mathbf{r}_i$ can be represented as $\mathbf{X}_i (s_i)= \mathbf{X}_i^* + s_i \mathbf{r}_i$, where $s_i$ is the signed distance along the viewing ray (not the positive distance as defined by the $t_i$). Then Equation (\ref{equ:costFunc_continuous}) can be reformulated as
\begin{equation}
\label{equ:costFunc_analysis}
\underset{\mathbf{s}} { \text{min} }
\sum_{(i,j)\in{\mathbf{p}}}{\|\mathbf{d}_{i,j}\times(\mathbf{X}_i(s_i)-\mathbf{X}_j(s_j))\|_2^2 + \lambda\|\mathbf{X}_i(s_i)-\mathbf{X}_j(s_j)\|_2^2}, \\
\end{equation}
where $\mathbf{s}=[s_1, \dots, s_N]$. Though $s_i$ is signed distance and $t_i$ is positive distance, minimizing Equation (\ref{equ:costFunc_continuous}) and Equation (\ref{equ:costFunc_analysis}) still output the same 3D point positions, as long as the computed 3D points in Equation (\ref{equ:costFunc_analysis}) are in front of the camera centers. We will see that this is normally true, since the computed 3D points are typically close to their ground truth position if the system is well-conditioned.

We denote the solution of Equation (\ref{equ:costFunc_analysis}) as $\mathbf{s}^\text{opt}$. The true 3D points are ideally reconstructed if $\mathbf{s}^\text{opt}=0$, since $\mathbf{X}_i(0)$ equals to $\mathbf{{X}}_i^*$ given $\mathbf{s}^\text{opt}=0$.
More specifically, $\mathbf{s}^\text{opt}$ equals the signed Euclidean distance between the 3D points produced by Equation (\ref{equ:costFunc_continuous}) and the ground truth $X_i^*$.
Therefore, $\|\mathbf{s}^\text{opt}\|$ is the Euclidean error of the estimated 3D points by Equation \ref{equ:costFunc_continuous}.
In the remainder of this section, we further analyze in which situations $\|\mathbf{s}^\text{opt}\|$ is small to better understand the quality of the estimated 3D points. %achieved approximation.

The minimum value of Equation (\ref{equ:costFunc_analysis}) is achieved at the point where the first derivative relative to $\mathbf{s}$ equals 0. This produces a linear equation system $\mathbf{A}\mathbf{s}^\text{opt}= \mathbf{b}$, where the $i$th row and $j$th column of matrix $\mathbf{A}$ is
\begin{equation}
A_{ij}= \begin{cases}
%\mathbf{r}_i\cdot\mathbf{r}_j, & \text{if } (i,j)\in T \text{ and } i\neq j \\
\lbrack(\mathbf{r}_i\cdot\mathbf{d}_{i,j})\mathbf{d}_{i,j}-(1+\lambda)\mathbf{r}_i\rbrack\cdot\mathbf{r}_j  & \text{if } i\neq j \text{ and } (i,j)\in \mathbf{p}\\
0, & \text{if } i\neq j \text{ and } (i,j)\notin \mathbf{p}\\
\sum_{(i,k)\in{\mathbf{p}}}{\lbrack 1+\lambda - (\mathbf{r}_i\cdot\mathbf{d}_{i,k})^2 \rbrack } & \text{if } i=j.
\end{cases}
\label{equ:A}
\end{equation}
The $i$th element of vector $\mathbf{b}$ is
\begin{equation}
b_i=\sum\nolimits_{(i,k)\in{\mathbf{p}}}(\mathbf{X}_k^*-\mathbf{X}_i^*)\cdot \lbrack(1+\lambda)\mathbf{r}_i - (\mathbf{r}_i\cdot \mathbf{d}_{i,k})\mathbf{d}_{i,k} \rbrack.
\label{equ:b}
\end{equation}
Equation (\ref{equ:A}) and Equation (\ref{equ:b}) have the following interesting properties:

\begin{figure}
\centering
%\begin{subfigure}[b]{0.3\textwidth}
\subfloat[$\lambda=0$]{
\centering
    \includegraphics[height=0.24\textheight]{chapter4/resource/analysis_1_cropped.pdf}
    \label{fig:b_lambda0}
}
\subfloat[$\lambda>0$]{
\centering
    \includegraphics[height=0.24\textheight]{chapter4/resource/analysis_2_cropped.pdf}
    \label{fig:b_lambdaBiggerthan0}
}
\caption[Illustration of the reconstructability given different weight $\lambda$.]{Plot of Equation (\ref{equ:b}) with $\lambda=0$ and $\lambda>0$.}
\label{fig:b}
\end{figure}

\begin{enumerate}
\item If $\mathbf{b}$ is 0, $\mathbf{s}^\text{opt}$ equals 0, which means the solution of Equation (\ref{equ:costFunc_continuous}) recovers the ground truth 3D points. There are a few situations where $\mathbf{b}$ equals 0. (1) In the case of a static object $\mathbf{X}_i^* = \mathbf{X}_k^*$, $\mathbf{b}$ equals 0 based on Equation (\ref{equ:b}). (2) Careful observation reveals that if $\lambda$ is set to $0$, in Equation (\ref{equ:b}) the vector $(1+\lambda)\mathbf{r}_i - (\mathbf{d}_{i,k}\cdot \mathbf{r}_i)\mathbf{d}_{i,k}$ is perpendicular to vector $\mathbf{X}_i^*-\mathbf{X}_k^*$ (Figure \ref{fig:b_lambda0}), hence $b_i=0$. However, we will show that with $\lambda=0$, the linear system $\mathbf{A}\mathbf{s}= \mathbf{b}$ is unstable due to the high condition number of matrix $\mathbf{A}$. (3) Furthermore, when $\lambda$ increases from 0, the two vectors slowly deviate from being perpendicular, as shown in Figure \ref{fig:b_lambdaBiggerthan0}. Therefore, $b_i$ is likely to be small if $\lambda$ is close to 0.

\item Since we can not control 3D positions and there are typically small measurement errors in $\mathbf{d}_{ij}$, $\mathbf{b}$ does not exactly equal to 0. This can be regarded as a small disturbance of $\mathbf{b}$ around $\mathbf{0}$. For the linear system $\mathbf{A}\mathbf{s}^\text{opt}= \mathbf{b}$, one can think of the condition number $\kappa(\mathbf A)$ as being (roughly) the rate at which the solution, $\mathbf{s}^{\text{opt}}$, will change with respect to a change in $\mathbf{b}$. $\kappa(\mathbf A)$ is available because it solely depends on $\mathbf{r}_i$, $\mathbf{d}_{i,j}$ and $\lambda$, but not on the ground truth 3D points $\mathbf{X}^*$. Therefore, we can estimate the reliability of the reconstructed 3D points by computing $\kappa(\mathbf A)$. Moreover, we empirically found that the condition number of matrix $\mathbf{A}$ is inversely related to $\lambda$. The condition number shown in Figure \ref{fig:conditionNum_lamda} is computed using 100 random cameras, and averaged over 200 trials. We can see $\kappa(\mathbf A)$ is large if $\lambda$ is close to 0 and drops dramatically with small $\lambda$. Then, $\kappa(\mathbf A)$ decreases monotonically and slowly as $\lambda$ increases. In our experiments, we choose $\lambda=\frac{1}{15}$ as a balance of having good chance of small $\mathbf{b}$ without decreasing the stability of the linear system.
\end{enumerate}

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{chapter4/resource/conditionNum_lamda.pdf}
\caption[Illustration of the system condition number given different weight $\lambda$.]{$\kappa(\mathbf{A})$ given different $\lambda$.}
\label{fig:conditionNum_lamda}
\end{figure}

In conclusion, if the adjacency $\mathbf{p}$ is correctly found, the reconstructabililty of the object class trajectory mainly depends on the condition number of the linear system. Given the well-conditioned system and correct motion tangent $\mathbf{d}_{i,j}$, we are able to reconstruct the 3D positions close to the ground truth.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Object Detector and Motion Tangent Estimation}
\label{sec:face_detection}
Before presenting our experimental evaluation, we first briefly describe the particular object detector we use in our experiments.
Single-image-based object detection is a well studied problem in computer vision with a wide variety of methods readily available~\cite{Zhang2006Local,Dalal2005HOG,lsvm-pami}. Similarly, there is a large number of motion tangent estimation methods in the literature~\cite{Blanz2003face,Gu20063D,jain2010fddb,jones2003fast}.
%While a combination of those methods would yield the desired detections and motion tangents,

For images containing large faces, we opt for leveraging the method that jointly determines the face position and its motion tangent direction~\cite{Xiangxin_CVPR12}.
In our experiments,
the detection threshold is set to $-0.35$ to avoid false detections, as the false alarm may disturb our algorithm. Our chosen detectors provide a motion tangent of object $i$ that is  quantized every $\theta=15^{\circ}$ in the range of $-90^{\circ}$ and $90^{\circ}$.


For cars and pedestrians with small faces in the images, we default to the deformable parts
detector~\cite{lsvm-pami,voc-release5}. %, which could also be used for the detection of cars. %This detector utilizes HOG features.
We used the pre-trained model with detection threshold $0.35$.
%The deformable part-based models are discriminatively learned using latent SVMs.
%We used the pre-trained pedestrian model from the INRIA person dataset~\cite{Dalal2005HOG} with detection threshold $0.35$.
% and the \en{ do we have a car experiment? If not we need to take the car out here} car model from the PASCAL dataset~\cite{pascal-voc-2010} with detection threshold $0.1$.
%We again chose higher detection thresholds to avoid false positives. Next we will describe the experimental evaluation of our \jost.
The motion tangent of the pedestrians and cars are estimated using the 3D point cloud (output of VisualSFM) of the background wall by assuming the dynamic objects move parallel to the wall. This is normally true in Manhattan scenes. Some of the detection results are shown in Figure \ref{fig:detection}.

\begin{figure}[t]
\centering
\begin{tabular}{c c c}
\includegraphics[width=0.3\textwidth]{chapter4/resource/ped_seq_3.jpg} & \includegraphics[width=0.3\textwidth]{chapter4/resource/IMG_3375.jpg} &
\includegraphics[width=0.3\textwidth]{chapter4/resource/IMG_3000-alldect.jpg}  \\
\includegraphics[width=0.3\textwidth]{chapter4/resource/ped_seq_4.jpg} &
\includegraphics[width=0.3\textwidth]{chapter4/resource/IMG_3365.jpg} &
\includegraphics[width=0.3\textwidth]{chapter4/resource/IMG_3073-alldect.jpg}
\end{tabular}
\caption[Example of the object detection output and the estimated motion tangent.]{Detected objects and estimated motion tangents using different detectors.}
\label{fig:detection}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
We evaluate our algorithm on both synthetic and real datasets.
The GMST algorithm used in our method \cite{Ferreira_ESWA2012} searches the hypothesis space, which stops iterating when either the GMST cost is under a preset value, or the run time reaches a preset limit. For all experiments, we only use the time limit to stop searching, given the lack of an adequate {\em a priori} approximation of the true  GMST cost for each dataset.
\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}  \hline
               & single line & T junction & double lines & half circle & sine wave & cross \\
  \hline
  $\text{error}_A$ & 0.5963 & 1.9688 & 1.5169 & 2.3751  & 2.3705 & 3.4111\\
  \hline
  $\text{error}^*_A$ & 0.4263 & 1.9148 & 1.4982 & 2.3340  & 2.3516 & 3.4030 \\
  \hline
  $\text{error}_B$ & 0.2151 & 0.2126 & 0.7824 & 0.2281& 0.2578 &0.2251 \\
  \hline
  $\text{error}^*_B$ & 0.0287 & 0.0944 & 0.7692 & 0.1074 & 0.2305 & 0.1308\\
  \hline
\end{tabular}
\caption[Quantitative evaluation of JOST on synthetic data]{ The table shows the average errors. The subscript represents camera setup. The absence of an asterisk represents the GMST algorithm output, and the asterisk is the refined output of Equation (\ref{equ:costFunc_continuous}). Notice that for the ground truth 3D points, the average distance between every pair of nearest points equals 1. }
\label{fig:syntheticDataResult}
\end{table}

\begin{figure}
\centering
    \includegraphics[width=0.99\textwidth]{chapter4/resource/allfig_2.pdf}
\caption[Example output of JOST on synthetic data]{Example results for line path, T-junction path, half circle and crossed paths.}
\label{fig:synthetic}
\end{figure}

\textbf{Synthetic datasets}. Our first experiment uses synthetic data, with six different \oct~shapes on a plane, including a single line path, a T-junction path, a path with two parallel lines, a half circle path, a sine-wave-shaped path, and a cross-shaped path. To have a sense of the output errors, we normalize the 3D points so that the average distance between every pair of nearest points equals 1.  

The virtual cameras are randomly generated around the 3D object points with two different configurations. In camera configuration A, all the camera centers stay in the same plane as the 3D points, which is more difficult since each viewing ray may intersect the ground truth path several times. In camera configuration B, the camera centers are set randomly off the plane, with the angle between the viewing ray and the plane being at most $10^\circ$ and the camera distance being 2-3 times the length of the path. 

We choose $k=100$ uniformly distributed discrete 3D hypotheses $\mathbf{X}_i^o$  along each viewing ray $\mathbf{X}_i$ in a range that contains the ground truth 3D point. The size of the range is set as 1.5 times the length of the path. Notice that while the ground truth 3D point lie in the range for a given image, there is no guarantee that any of the discrete hypothesis samples $\mathbf{X}_i^o$ will exactly match the true depth. 

Errors are measured using the Euclidean distance between the estimated 3D points and the ground truth. We run 32 instances for each shape with randomly generated virtual cameras.
The average errors over the 32 instances for each shape category are listed in Table \ref{fig:syntheticDataResult}. We report both the errors of the GMST output and the errors after the continuous refinement using Equation (\ref{equ:costFunc_continuous}).  Table \ref{fig:syntheticDataResult} shows our  continuous refinement always improves the reconstruction accuracy over the GMST approximation.  The results demonstrate    {\it off-plane} cameras yield improved results than {\it in-plane}  cameras  for  complex paths (e.g. crossed paths), due to the multiplicity of ray-to-path intersections. In these cases, the GMST solution has a more complex search space and yields a sub-optimal solution. However, the condition number of the linear system does not vary significantly across configurations. Figure \ref{fig:synthetic} shows the estimated 3D points overlaid onto the ground truth.

\begin{figure}
\centering
\subfloat[Reconstruction of cars and pedestrians on a street]{
\centering
    \includegraphics[height=0.18\textheight]{chapter4/resource/snapshot03.jpg}
    \includegraphics[height=0.18\textheight]{chapter4/resource/snapshot05.jpg}
}\\
\subfloat[Reconstruction of people walking on a T-junction path]{
    \includegraphics[height=0.21\textheight]{chapter4/resource/tjunction1.jpg}
    \includegraphics[height=0.21\textheight]{chapter4/resource/tjunction2.jpg}
}
\caption[Example reconstruction results on two real datasets.]{Two views for each of the reconstructed results.}
\label{fig:reconstructed}
\end{figure}
\textbf{Real datasets}. We evaluate our method on two image datasets  registered by VisualSFM \cite{WuVSFM}. The detection confidence threshold  is set high in order to decrease the false alarm rate. However, a very small amount of false alarms were purged manually, as they may affect the reconstruction.  We sample 100 samples along the viewing ray in the range $[0,far]$, where $far$ is  estimated using the model scale. The run time for each object class trajectory is set to 3 hours.

The first dataset captures random pedestrians walking on a sidewalk, plus random cars driving on an adjacent road. It contains 135 images with 82 valid car detections and 137 valid pedestrian detections. The scene and the reconstructed object class trajectory are shown in Figure \ref{fig:franklin_Recon}. %and the images are taken on the other side of the street at different time.
The second dataset captures several people who are walking on a T-junction shaped path at the corner of a building. It contains 47 images with 66 valid detections. Using the camera positions, we convert the face directions into the global coordinate system to obtain the motion tangents $\mathbf d_i$ of the moving people. For illustration, we construct the background static scene using CMPMVS \cite{JAN}. The general 3D human and car mesh models are inserted into each of the estimated 3D positions. We show different views of the reconstructed results in Figure \ref{fig:reconstructed}.

\begin{figure}
\centering
\begin{tabu}{ |[2pt]c |[2pt] c |[2pt]}
\tabucline[2pt]{-}
\includegraphics[width=0.32\textwidth]{chapter4/resource/googlescholar.jpg} &
\includegraphics[width=0.32\textwidth]{chapter4/resource/car_ped_franklin_top.pdf}  \\
\tabucline[2pt]{-}
\tabucline[2pt]{-}
\includegraphics[width=0.32\textwidth]{chapter4/resource/cleanFrame083.jpg} & 
\includegraphics[width=0.32\textwidth]{chapter4/resource/Frame_083_crop.jpg}  \\
\tabucline[2pt]{-}
\includegraphics[width=0.32\textwidth]{chapter4/resource/cleanFrame084.jpg} &
\includegraphics[width=0.32\textwidth]{chapter4/resource/Frame_084_crop.jpg} \\
\tabucline[2pt]{-}
\includegraphics[width=0.32\textwidth]{chapter4/resource/cleanFrame085.jpg} & 
\includegraphics[width=0.32\textwidth]{chapter4/resource/Frame_085_crop.jpg} \\
\tabucline[2pt]{-}
\includegraphics[width=0.32\textwidth]{chapter4/resource/cleanFrame086.jpg} &
\includegraphics[width=0.33\textwidth]{chapter4/resource/Frame_086_crop.jpg}\\
\tabucline[2pt]{-}
\end{tabu}
\caption[Illustration of pedestrian reconstruction results.]{Top row: An aerial image showing the scene and a figure showing the cameras and reconstructed cars and pedestrians. Bottom four rows: Four pedestrian detections (shown in yellow rectangles) and the poses of the corresponding cameras. These four pedestrians are adjacent in the reconstructed object class trajectory. Notice that the second and the third images are the same image but with different detections.}
\label{fig:franklin_Recon}
\end{figure}

\section{Conclusion}
We target the problem of reconstructing the 3D positions of dynamic objects from a set of unordered images, with the assumption that the objects of the same class move in a common path in the scene. 
We propose a framework of joint object class sequencing and trajectory triangulation and solve the associated non-convex optimization problem through a new discrete-continuous optimization scheme based on the generalized minimum spanning tree (GMST).
The promising results on synthetic and real datasets demonstrate the solvability of the difficult problem and the effectiveness of our approach.

