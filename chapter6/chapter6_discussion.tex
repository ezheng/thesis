%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion}
\label{ch:discussion}
This dissertation presents three works for the problems in static scene reconstruction and dynamic object reconstruction. In Chapter \ref{ch:patchmatch}, we proposed a framework of joint view selection and depthmap estimation. The experiments on large Internet collected photos demonstrates its efficiency and robustness. In Chapter \ref{ch:jost} and Chapter \ref{ch:video_l1}, we solved the problems of dynamic object reconstruction from unstructured images and unsyncthronized videos, respectively. In solving these two problems, our main effort focused on 3D reconstruction without sequencing information. We showed effectiveness of the approaches by testing on synthetic and real datasets. In this section, we discuss the possible extensions of our works, as well as the potential future research directions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extensions to PatchMatch-based joint view selection and depthmap estimation}
\label{sec:patchmatch_extensions}

Though our method in Chapter \ref{ch:patchmatch} significantly outperforms existing methods on Internet collected photos \cite{Goesele07} and achieves the state-of-the-art accuracy on standard datasets collected under a controlled lab environment \cite{Strecha08}. The accuracy of the method can be further improved by incorporating some standard techniques into our framework. Next, we discuss each of the techniques in detail.

In our method, we use the fronto-parallel planes to warp color patches in the reference image onto other source images to perform a color consistency check. It has been shown the plane orientation affects the reconstruction accuracy \cite{Gallup07,FURUKAWA_PAMI2010}. Ideally, the plane orientation should be the same as the real surface normal, which is unknown before reconstruction. To address this issue, we can include the surface normals as unknown variables in our framework. Specifically, the unknown normal directions are propagated to the neighboring pixels in addition to the depths \cite{patchMatchStereo1}. This scheme is able to further improve reconstruction quality on the regions having large angles with the camera viewing directions (e.g. the ground), but at the cost of increased computational complexity.

Another issue relating to color patches arises if the pixels in a patch cover scenes of significantly different depths, which typically occurs at the boundary of object surfaces. 
In stereo, the correspondences among multiple images are found by checking the color consistency.
To improve the robustness for the color consistency measure between two pixels, current local methods (\ie methods having no smoothness term between neighboring pixels in the depthmap) typically compare the two patches around the pixels. The method present in Chapter \ref{ch:patchmatch} applies normalized cross correlation (NCC) as a metric to measure the color consistency, where each pixel in the patch contributes equally to the measure. 
However, this is likely to produce swollen/fat boundary effect in the depthmap, since the use of a plane for patch warping assumes all pixels in the patch have the same depth and normal, and this assumption breaks at the boundary of object surfaces. 
Therefore, when compare two patches, the pixels with the same depth as the central pixel should be given higher weight than other pixels with different depths. To achieve this, one heuristic but empirically effective solution is to use adaptive weights for each pixel within the patch, with the weights both propotional to the color similarity and the spatial proximity relative to the patch's center on the reference image \cite{Yoon06adaptivesupport_weight}. 

Another extension to our work is to handle cameras with small baselines. In stereo methods, small baselines usually lead to unstable and inaccurate results \cite{Hartley2004}. Since the large set of Internet collected photos is typically taken at certain spots of interest, it is very likely some of the images have very small or zero baselines. Our framework selects images based on color consistency, and the images with small baselines will generally be selected because the color consistency is always high regardless of the depth hypothesis. To address this issue, the angle of two viewing rays given a depth hypothesis should be tested to prevent invalid triangulation \cite{Gallup08}. Specifically, if the angle of two viewing rays is below a predefined threshold, the corresponding source image and the depth hypothesis should be deemed  unreliable.

Another issue related to depth estimation comes from homogeneous color regions (\emph{i.e.}~ image regions lacking a textured color patter).
All existing methods based on local color consistency checks fail on these regions. To handle this problem, I believe it is necessary to incorporate the semantic knowledge of the scene rather than to just rely on low-level colors. This inevitably requires introducing machine learning techniques into the stereo problem. However, incorporating camera parameters into a machine learning framework is difficult, since the testing data and training data often have different camera parameters. Though there are many single-image depth estimation approaches based on supervised machine learning  \cite{Hoiem_CGRAPH2005,Saxena_IJCV2008,eigen2014depth,Liu2014,zhuo2015indoor}, still much work has to be done to incorporate such techniques into multiview stereo methods for more accurate depth estimation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Extensions to JOST}

The method presented in Chapter \ref{ch:jost} uses object detection output as features, and the object lies along the viewing ray passing the 2D features. However, the outlier detections may prevent the algorithm from finding the correct object class trajectories. One way to manage this problem, as is done in Chapter \ref{ch:jost}, is to raise the detection threshold to suppress the false alarm rate, at the cost of increasing miss detections. Another possible way is to embed our method in a RANSAC framework \cite{Hartley2004} to remove outliers. Specifically, a subset of randomly sampled detections is used to triangulate the trajectory, and count the number of remaining detections censuses with the trajectory as inliers. Repeat this process to find the trajectory with the largest number of inliers. However, this scheme is computational intensive if the ratio of outliers is large, considering that running trajectory triangulation given a subset of detections is time-consuming. 

Efficiency is another issue for our approach. In our method, the nonconvex problem is solved in a discrete-continuous scheme, and the discrete step involves solving a NP-hard GMST problem. The efficiency of solving a GMST problem can be attained by reducing the complexity of the K-partite graph. A complete multipartite graph has all the nodes in one partite set connecting to all the nodes in another partite set. The computational complexity of finding GMST will be lowered down if the number of edges and nodes of the graph is reduced. 
To achieve this, a prior knowledge, if available, can be incorporated easily. 
For instance, if it is known that two specific detected objects are farther away in 3D space, then all the edges connecting the associated two sets of nodes can be safely removed, since these two objects are not neighboring in the object class trajectory. Moreover, if the scene model size and the real object size is available, then the size of the detection windows can be used to roughly estimate a tight depth range of the dynamic objects, which helps reduce the number of nodes in the partite sets and hence the number of edges in the graph.

\subsection{Extensions to dynamic object reconstruction from unsynchronized videos}

% obtaining correspondences.
% possible dense reconstruction.
In Chapter \ref{ch:video_l1}, we obtain the 2D correspondences across images manually as input for our approach. 
This step can be automated by optical flow \cite{brox2004high} or graph match based matching algorithms \cite{yan2015multi,Yan_2015_CVPR}. Moreover, optical flow can produce dense correspondences so that we can reconstruct dense 3D points for the dynamic objects. 

The method presented in Chapter \ref{ch:jost} and Chapter \ref{ch:video_l1} requires a static background scene present in the image so that structure from motion can use it for camera registration. However, the crowd sourced data may have dynamic objects as the main focus and lack the content of the background scenes. This comes an open question of how to register cameras given non-current captures of dynamic objects. Considering the importance and difficulity of this problem, it can be a possible future research direction.

Moreover, to the best of our knowledge, the work in Chapter \ref{ch:video_l1} is the first self-representation framework for dynamic object reconstruction. That is, each temporally varied shape can be represented by a linear combination of a few other shapes at different time instances, given the smooth motion of dynamic objects. This self-representation constraint has potential to be used to solve the NRSFM problems. Compared to most of the existing works for NRSFM, where the assumption that any shape is a linear combination of $K$ shape bases is applied, our self-representation constraint is more intuitive and possibly lead to better reconstruction results.
