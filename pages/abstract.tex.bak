% !TEX root = ../thesis_heinly.tex

\begin{center}
\vspace*{46pt}
\currentpdfbookmark{ABSTRACT}{bk:abstract}
\textbf{ABSTRACT}
\vspace{4pt}

\begin{singlespace}
\thesisauthor: \thesistitle \\
(Under the direction of \thesisadvisors)
\vspace{10pt}
\end{singlespace}
\end{center}


With the bursting number of images/videos available on the Internet, it has been of interest to perform 3D reconstruction of a scene from these data for further applications. 
Though 3D reconstruction, a process that recovers the 3D information of a scene via images, is a traditional topic in computer vision, some of the challenging problems still remain partially solved or even unsolved. This is especially true when using the crowd-sourced data with heterogeneous properties, such as images with random occlusions and videos with unknown temporal overlap. Moreover, most of existing methods only target on the static scene reconstruction, but fail on the dynamic part due to more challenges. To push forward the research frontier for image based 3D modeling, this dissertation focuses on the problem of static as well as dynamic object reconstruction from images or videos. 

Static scene depthmap estimation determines a view-dependent depthfield by leveraging
the local photoconsistency of a set of overlapping images observing a common scene. 
While achieving high-quality depthmap using images taken under controlled environment is difficult, the task with heterogeneous crowd-sourced data even brings more challenges. 
We propose a method that estimates the depthmaps of the crowd-sourced images, and demonstrate its accuracy, robustness, and scalability on large number of internet collected photos.
 
Comparing to static scene reconstruction, the dynamic object reconstruction from monocular images is fundamentally ambiguous without any assumption, as the unitary image observation impedes valid 3D triangulation.  Assuming that the dynamic objects of the same class (e.g.~cars or pedestrians) move in a common path in the real world, we develop a method that estimates the 3D positions of the dynamic objects from unstructured monocular images. The experiments on both synthetic and real datasets illustrate the effectiveness of our approach.

We then further tackle the problem of dynamic object reconstruction, but using unsynchronized videos capturing the same dynamic event instead of using images as input. This is an interesting problem since captures using unsynchronized videos are common in the real world due to the increased availability of portable capture devices. To tackle the difficulty arising from unknown temporal overlap among video streams, we propose a self-expressive dictionary learning framework, where the dictionary is defined as the temporally varying structures. The experimental results show that this approach solves the previously unsolved problem.
%The method is evaluated with large number of real motion capture datasets and real imagery. 


\clearpage
